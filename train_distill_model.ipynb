{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 115049 files belonging to 8 classes.\n",
      "Using 92040 files for training.\n",
      "Using 23009 files for validation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tf_stft(x, y):\n",
    "    # Ensure correct shape\n",
    "    x = tf.squeeze(x, axis=-1)\n",
    "\n",
    "    # Create a Spetrogram\n",
    "    stft = tf.signal.stft(x, \n",
    "                          frame_length=1024, frame_step=512,\n",
    "                          window_fn=tf.signal.hamming_window)\n",
    "    # Take the same range of frequencies as in the pretrained model\n",
    "    stft = stft[:, :, :int(stft.shape[2] * 5000 / (x.shape[1] // 2))]\n",
    "    stft = tf.math.abs(stft)\n",
    "    # Resize to the same shape as the input to the pretrained model\n",
    "    stft = tf.repeat(stft, 16, axis=1)\n",
    "    stft = tf.transpose(stft, perm=[2,1,0])[::-1]\n",
    "    stft = tf.image.resize(stft, (369, 496))\n",
    "    stft = tf.transpose(stft, perm=[2,0,1])\n",
    "\n",
    "    # To 3-channel image (again, the same as the input to the pretrained model)\n",
    "    stft = tf.expand_dims(stft, -1)\n",
    "    stft = tf.image.grayscale_to_rgb(stft)\n",
    "    \n",
    "    return stft, tf.one_hot(y, 8)\n",
    "\n",
    "train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory='train_audio_for_distill',\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    seed=0,\n",
    "    output_sequence_length=16000,\n",
    "    subset='both')\n",
    "\n",
    "# transform all audios to spectrograms\n",
    "train_ds = train_ds.map(tf_stft)\n",
    "val_ds = val_ds.map(tf_stft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 367, 494, 8)       224       \n",
      "                                                                 \n",
      " normalization (Normalizatio  (None, 367, 494, 8)      17        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 183, 247, 8)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 181, 245, 16)      1168      \n",
      "                                                                 \n",
      " normalization_1 (Normalizat  (None, 181, 245, 16)     33        \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 90, 122, 16)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 88, 120, 32)       4640      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 88, 120, 32)       0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 337920)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               43253888  \n",
      "                                                                 \n",
      " normalization_2 (Normalizat  (None, 128)              257       \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 1032      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 43,261,259\n",
      "Trainable params: 43,260,952\n",
      "Non-trainable params: 307\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load model but initialize with new weights\n",
    "with tf.device('cpu'):\n",
    "    pretrained_model = tf.keras.models.load_model('my_model', compile=False)\n",
    "    \n",
    "with tf.device('/gpu:2'):\n",
    "    distilled_model = tf.keras.models.clone_model(pretrained_model)\n",
    "\n",
    "    distilled_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.CategoricalAccuracy()]\n",
    "    )\n",
    "\n",
    "    print(distilled_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1439/1439 [==============================] - 161s 108ms/step - loss: 1.0546 - categorical_accuracy: 0.6642 - val_loss: 0.6701 - val_categorical_accuracy: 0.7646\n",
      "Epoch 2/10\n",
      "1439/1439 [==============================] - 152s 105ms/step - loss: 0.5925 - categorical_accuracy: 0.7879 - val_loss: 0.6232 - val_categorical_accuracy: 0.7775\n",
      "Epoch 3/10\n",
      "1439/1439 [==============================] - 148s 103ms/step - loss: 0.4082 - categorical_accuracy: 0.8554 - val_loss: 0.6962 - val_categorical_accuracy: 0.7749\n",
      "Epoch 4/10\n",
      "1439/1439 [==============================] - 141s 97ms/step - loss: 0.2750 - categorical_accuracy: 0.9048 - val_loss: 0.7937 - val_categorical_accuracy: 0.7739\n",
      "Epoch 5/10\n",
      "1439/1439 [==============================] - 137s 95ms/step - loss: 0.1988 - categorical_accuracy: 0.9317 - val_loss: 0.8545 - val_categorical_accuracy: 0.7688\n",
      "Epoch 6/10\n",
      "1439/1439 [==============================] - 138s 96ms/step - loss: 0.1590 - categorical_accuracy: 0.9475 - val_loss: 1.0394 - val_categorical_accuracy: 0.7693\n",
      "Epoch 7/10\n",
      "1439/1439 [==============================] - 132s 92ms/step - loss: 0.1358 - categorical_accuracy: 0.9551 - val_loss: 0.9977 - val_categorical_accuracy: 0.7706\n",
      "Epoch 8/10\n",
      "1439/1439 [==============================] - 131s 91ms/step - loss: 0.1170 - categorical_accuracy: 0.9626 - val_loss: 1.0304 - val_categorical_accuracy: 0.7847\n",
      "Epoch 9/10\n",
      "1439/1439 [==============================] - 130s 90ms/step - loss: 0.1010 - categorical_accuracy: 0.9673 - val_loss: 1.0878 - val_categorical_accuracy: 0.7602\n",
      "Epoch 10/10\n",
      "1439/1439 [==============================] - 133s 92ms/step - loss: 0.0915 - categorical_accuracy: 0.9699 - val_loss: 1.1611 - val_categorical_accuracy: 0.7646\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:2'):\n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "    EPOCHS = 10\n",
    "    history = distilled_model.fit(train_ds,\n",
    "                        epochs=EPOCHS,\n",
    "                        validation_data=val_ds,\n",
    "                        verbose=1\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: distilled_model/my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: distilled_model/my_model/assets\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p distilled_model\n",
    "distilled_model.save('distilled_model/my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
